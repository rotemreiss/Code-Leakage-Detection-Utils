#!/usr/bin/env bash

# Prerequisites
# https://github.com/eth0izzle/shhgit

KEYWORD=$1

# CHANGE THE LOCAL DIR FOR WHATEVER YOU WANT.
LOCAL_DIR="/tmp/dockerhub-secrets"

SHHGIT_CONFIG="$HOME/shhgit-config/"

save_image_manifest() {
	IMAGE=$1
	TAG=$2

	AUTH="https://auth.docker.io"
	SERV="registry.docker.io"
	
	TOKEN=$(\
	  curl \
	  --silent \
	  --location \
	  "${AUTH}/token?service=${SERV}&scope=repository:${IMAGE}:pull" \
	  | jq -r .token \
	)

	REG="https://index.docker.io/v2"
	SCHEMA="application/vnd.docker.distribution.manifest.list.v2+json"
	curl \
	  --silent \
	  --location \
	  --header "Authorization: Bearer ${TOKEN}" \
	  --header "Accept: ${SCHEMA}" \
	  "${REG}/${IMAGE}/manifests/${TAG}" > manifest-raw.json

	cat manifest-raw.json | jq -r ".history[].v1Compatibility" | jq -r ".container_config.Cmd[]" | tail -r > layers.txt
	echo "[-] Fetched the manifest file of ${IMAGE}:${TAG}."
}

save_image() {
	IMAGE=$1
	# Get the layers data as well. - Replace with Rotem's tool.
	docker pull $IMAGE -q
 	docker save $IMAGE -o d.tar
 	tar -xf d.tar
 	# Now extract the layers archives.
 	find . -name "*.tar" -mindepth 2 -exec sh -c 'tar xf {} -C $(dirname {})' \;
 	echo "[-] Extracted image to $(pwd)."


	# echo "[-] Cleaning up image"
	docker rmi $IMAGE > /dev/null 2>&1
}


fetch_image() {
	IMAGE=$1

	echo "[-] Fetching $IMAGE."
	
	# Get only the first and last tags at the moment.
	IMAGE_TAGS=$(\
		curl -s \
		"https://hub.docker.com/v2/repositories/${IMAGE}/tags/?page_size=100&page=1" \
		| jq -r '.results[0].name, .results[-1].name' \
		| sort -u \
		)
	
	echo "[-] Fetching $(echo $IMAGE_TAGS | wc -w) tags."

	echo "[-] Fetching layers."

	for IMAGE_TAG in $IMAGE_TAGS; do
		IMAGE_DIR=`echo ${IMAGE}:${IMAGE_TAG} | sed "s%/%--%"`

		# Get the manifest (commands).
	 	mkdir -p $IMAGE_DIR
	 	cd $IMAGE_DIR
		
		save_image_manifest ${IMAGE} ${IMAGE_TAG}

	 	# echo "[-] Extreacting ENV from the manifest.."
		grep "ENV" layers.txt > ENV.txt
		# echo "[-] Extreacting MAINTAINER from the manifest.."
		grep "MAINTAINER" layers.txt > MAINTAINERS.txt

		# echo "[-] Scanning for interesting layers. Hold on.."
		
		# @todo Add CI tools files.
		grep -i -e "appSettings\.json" \
			-e "\.htpasswd" \
			-e "\.pypirc" \
			-e "\.env" \
			-e "settings\.xml" \
			-e "settings\.php" \
			-e "\.npmrc" \
			-e "pip\.ini" \
			-e "pip\.conf" \
			-e "\.properties" \
			-e "\.ovpn" \
			-e "\id_rsa" \
			-e "\.properties" \
			-e "web\.config" \
			-e "\.conf" \
			-e "\.pem" \
			-e "\.yaml" \
			-e "\.yml" \
			-e "\.ini" \
			-e "\.cscfg" \
			-e "\.rdp" \
			-e "\.jks" \
			-e "\.psafe3" \
			-e "secret_token\.rb" \
			-e "carrierwave\.rb" \
			-e "omniauth\.rb" \
			-e "settings\.py" \
			-e "\.agilekeychain" \
			-e "\.gnucash" \
			-e "jenkins\.plugins\.publish_over_ssh\.BapSshPublisherPlugin\.xml" \
			-e "credentials\.xml" \
			-e "\.kwallet" \
			-e "LocalSettings\.php" \
			-e "\.tblk" \
			-e "Favorites\.plist" \
			-e "configuration\.user\.xpl" \
			-e "knife\.rb" \
			-e "proftpdpasswd" \
			-e "robomongo\.json" \
			-e "filezilla\.xml" \
			-e "recentservers\.xml" \
			-e "ventrilo_srv\.ini" \
			-e "terraform\.tfvars" \
			-e "\.ppk" \
			-e "heroku\.json" \
			-e "mongoid\.yml" \
			-e "salesforce\.js" \
			-e "\.netrc" \
			-e "\.pgpass" \
			-e "\.irb_history" \
			-e "\.dbeaver-data-sources\.xml" \
			-e "\.muttrc" \
			-e "\.s3cfg" \
			-e "sftp-config" \
			-e "\.trc" \
			-e "config\.inc\.php" \
			-e "config\.php" \
			-e "keystore" \
			-e "keyring" \
			-e "netrc" \
			-e "\.tugboat" \
			-e "\.git-credentials" \
			-e "\.gitconfig" \
			-e "\.dockercfg" \
			-e "\.remote-sync.json" \
			-e "\.esmtprc" \
			-e "deployment-config\.json" \
			-e "\.ftpconfig" \
			-e "\Jenkinsfile" \
			layers.txt > REQUIRES_REVIEW.txt

		# Extract COPY and ADD layers
		grep -E "(COPY|ADD) (dir|file):[a-z0-9]{64}" layers.txt > ADD_COPY.txt

		# Find application dirs (might contain interesting code)
		grep -E "(COPY|ADD) (dir|file):[a-z0-9]{64} in (.)*\/app" \
			ADD_COPY.txt >> REQUIRES_REVIEW.txt

		# Extract some URLs and interesting URLs.
		grep -Eo "(ftp|ssh|http(s)?)://[a-zA-Z0-9./?=_%:-{}$\-]*" \
			layers.txt > URLS.txt
		grep -i -e "token" \
			-e "password" \
			-e "api_key" \
			-e "apikey" \
			URLS.txt > INTERESTING_URLS.txt

		if [ -s INTERESTING_URLS.txt ]
		then
		     echo "[+] Found interesting URLs in ${IMAGE}:${IMAGE_TAG}"
		fi

		# If we found interesting things, let's pull the image.
	 	if [ -s REQUIRES_REVIEW.txt ]
		then
		     echo "[+] Found interesting results in ${IMAGE}:${IMAGE_TAG}"
		     save_image "${IMAGE}:${IMAGE_TAG}"
		fi

		# Run from parent dir to get the path.
		shhgit -config-path "$SHHGIT_CONFIG" -silent -local . > SECRETS.txt
	 	if [ -s SECRETS.txt ]
		then
		     echo "[+] Found possible secrets in ${IMAGE}:${IMAGE_TAG} ðŸ””"
		     # Ring the bell!
		     tput bel
		fi

		cd ../
	done
}

mkdir -p $LOCAL_DIR
cd $LOCAL_DIR

mkdir -p $KEYWORD
cd $KEYWORD

echo "[-] Searching for Docker images with the keyword '$KEYWORD'."
IMAGES=$(docker search $KEYWORD --format "{{.Name}}" --filter is-official=false --no-trunc --limit 100)
IMAGES_COUNT=`echo $IMAGES | wc -w`
echo "[+] Found $IMAGES_COUNT images."

for IMAGE in $IMAGES; do
	fetch_image $IMAGE
done
